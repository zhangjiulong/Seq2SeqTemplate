{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A simple template for using the `seq2seq` module in TensorFlow v1.2+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### YJ Choe ([yj.choe@kakaobrain.com](mailto:yj.choe@kakaobrain.com))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tensorflow.python.layers import core as layers_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n",
      "1.13.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For TensorBoard summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/get_started/summaries_and_tensorboard\n",
    "def variable_summaries(name, var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "START_TOKEN = 0\n",
    "END_TOKEN = 1\n",
    "UNK_TOKEN = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the following parameters and data according to the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, batch_size = 500, 32\n",
    "n_epochs = 10\n",
    "\n",
    "vocab_size = 100\n",
    "max_timesteps = 16\n",
    "embedding_size = 20  # assume both encoding and decoding embeddings have the same size\n",
    "\n",
    "n_layers = 2  # number of layers of LSTM\n",
    "latent_size = 10  # num_units of LSTM\n",
    "\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_sequence_length(data):\n",
    "    dtype = data.dtype\n",
    "    sequence_length = 1 + tf.argmax(\n",
    "        tf.cast(tf.equal(data, tf.constant(END_TOKEN, dtype=dtype)), dtype),\n",
    "        axis=1\n",
    "    )\n",
    "    return tf.cast(sequence_length, dtype)\n",
    "    \n",
    "def generate_random_sequences(n, vocab_size, max_timesteps):\n",
    "    # avoid having multiple START_TOKEN's (vocab_size - 1)\n",
    "    # always have one START_TOKEN at the beginning \n",
    "    # and at least one END_TOKEN at the end\n",
    "    data = tf.cast(\n",
    "        tf.multinomial(logits=tf.log(tf.ones(shape=(n, vocab_size - 1))),\n",
    "                       num_samples=max_timesteps - 2),\n",
    "        tf.int32\n",
    "    )\n",
    "    data = tf.concat(\n",
    "        [tf.constant(START_TOKEN, shape=(n, 1)),\n",
    "         data + 1, \n",
    "         tf.constant(END_TOKEN, shape=(n, 1))  # in case END_TOKEN is not already present\n",
    "        ], \n",
    "        axis=1\n",
    "    )\n",
    "    sequence_length = compute_sequence_length(data)\n",
    "    return data, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 16) (500,)\n",
      "(500, 16) (500,)\n"
     ]
    }
   ],
   "source": [
    "# generate random data that always starts with START_TOKEN and ends with END_TOKEN\n",
    "x_data, x_data_length = generate_random_sequences(n, vocab_size, max_timesteps)\n",
    "y_data, y_data_length = generate_random_sequences(n, vocab_size, max_timesteps)\n",
    "print(x_data.shape, x_data_length.shape)\n",
    "print(y_data.shape, y_data_length.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 16) (32,)\n",
      "(32, 16) (32,)\n"
     ]
    }
   ],
   "source": [
    "x, x_length, y, y_length = tf.train.batch(\n",
    "    tensors=[x_data, x_data_length, y_data, y_data_length],\n",
    "    batch_size=batch_size,\n",
    "    num_threads=n_gpus,\n",
    "    enqueue_many=True,\n",
    "    dynamic_pad=True,\n",
    "    name='batched_xy'\n",
    ")\n",
    "print(x.shape, x_length.shape)\n",
    "print(y.shape, y_length.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assume that the embedding matrix is identical for the encoder and the decoder (e.g. question-answering within the same language). For machine translation tasks, specify two embedding matrices and apply them to `x` and `y` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a pre-trained embedding matrix (e.g. Word2Vec, GloVe, fastText) is available, replace `embedding` with it. Otherwise, set `trainable=True` when initializing `embedding_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20)\n"
     ]
    }
   ],
   "source": [
    "embedding = np.random.randn(vocab_size, embedding_size)\n",
    "# https://stackoverflow.com/questions/35687678/using-a-pre-trained-word-embedding-word2vec-or-glove-in-tensorflow\n",
    "embedding_matrix = tf.Variable(tf.constant(0.0, shape=(vocab_size, embedding_size)),\n",
    "                               trainable=False,   # if pre-trained\n",
    "                               name=\"embedding_matrix\")\n",
    "\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_size])\n",
    "embedding_init = embedding_matrix.assign(embedding_placeholder)\n",
    "\n",
    "#sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 16, 20)\n",
      "(32, 16, 20)\n"
     ]
    }
   ],
   "source": [
    "x_embedding = tf.nn.embedding_lookup(embedding_matrix, x)\n",
    "y_embedding = tf.nn.embedding_lookup(embedding_matrix, y)\n",
    "print(x_embedding.shape)\n",
    "print(y_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/transpose:0\", shape=(32, 16, 10), dtype=float32) (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(?, 10) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 10) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 10) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_5:0' shape=(?, 10) dtype=float32>))\n"
     ]
    }
   ],
   "source": [
    "# see also: `tf.contrib.rnn.LayerNormBasicLSTMCell`\n",
    "cells = [tf.nn.rnn_cell.LSTMCell(latent_size) for _ in range(n_layers)]\n",
    "# cells = [tf.nn.rnn_cell.DeviceWrapper(\n",
    "#     tf.nn.rnn_cell.ResidualWrapper(tf.nn.rnn_cell.LSTMCell(latent_size)),\n",
    "#     device='/gpu:%d' % i) for i in range(n_gpus)]\n",
    "\n",
    "encoder_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "    cells=cells\n",
    ")\n",
    "\n",
    "encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "    cell=encoder_cell,\n",
    "    dtype=tf.float32,\n",
    "    inputs=x_embedding\n",
    ")\n",
    "variable_summaries('encoder_outputs', encoder_outputs)\n",
    "variable_summaries('encoder_final_state', encoder_final_state)\n",
    "print(encoder_outputs, encoder_final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h, state_tuple = encoder_cell(x_embedding[:, 0, :], encoder_final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h == state_tuple[1].h  # same thing, unless cell has `num_proj` specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LSTMStateTuple(c=10, h=10), LSTMStateTuple(c=10, h=10))\n"
     ]
    }
   ],
   "source": [
    "cells = [tf.nn.rnn_cell.LSTMCell(latent_size) for _ in range(n_layers)]\n",
    "# cells = [tf.nn.rnn_cell.DeviceWrapper(\n",
    "#     tf.nn.rnn_cell.ResidualWrapper(tf.nn.rnn_cell.LSTMCell(latent_size)),\n",
    "#     device='/gpu:%d' % i) for i in range(n_gpus)]\n",
    "\n",
    "decoder_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "    cells=cells\n",
    ")\n",
    "print(decoder_cell.state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use `tf.contrib.seq2seq.BahdanauAttention` for additive attention\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "    num_units=latent_size,\n",
    "    memory=encoder_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# `tf.contrib.seq2seq.DynamicAttentionWrapper` before TF v1.2\n",
    "attention_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "    cell=decoder_cell,\n",
    "    attention_mechanism=attention_mechanism,\n",
    "    attention_layer_size=latent_size  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/tensorflow/issues/8833\n",
    "attention_zero_state = attention_cell.zero_state(\n",
    "    batch_size=batch_size, \n",
    "    dtype=tf.float32\n",
    ")\n",
    "attention_initial_state = attention_zero_state.clone(\n",
    "    cell_state=encoder_final_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, a `TrainingHelper` schedules whether the previous timestep's true output ($y_{t-1}$) or its predicted output ($\\hat{y}_{t-1}$) is fed into the next decoder ste where we predict $y_t$. [[paper]](https://arxiv.org/pdf/1506.03099.pdf) During prediction/testing, the true output is no longer available, so we use a non-training `Helper` (such as `GreedyEmbeddingHelper`) to always feed in the predicted output from the previous timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "    inputs=y_embedding,\n",
    "    sequence_length=y_length\n",
    ")\n",
    "prediction_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "    embedding=embedding,\n",
    "    start_tokens=tf.constant(START_TOKEN, shape=(batch_size, )),\n",
    "    end_token=END_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "    cell=attention_cell,\n",
    "    helper=training_helper,\n",
    "    initial_state=attention_initial_state,\n",
    "    output_layer=layers_core.Dense(vocab_size,\n",
    "                                   activation=tf.nn.sigmoid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_outputs, final_state, final_sequence_lengths = \\\n",
    "    tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder=decoder\n",
    "    )\n",
    "\n",
    "variable_summaries('final_rnn_outputs', final_outputs.rnn_output)\n",
    "variable_summaries('final_cell_state', final_state.cell_state)\n",
    "variable_summaries('final_attention', final_state.attention)\n",
    "variable_summaries('final_alignments', final_state.alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'decoder/transpose:0' shape=(32, ?, 100) dtype=float32>,\n",
       " <tf.Tensor 'decoder/transpose_1:0' shape=(32, ?) dtype=int32>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `final_outputs` has type `tf.contrib.BasicDecoderOutput`\n",
    "final_outputs.rnn_output, final_outputs.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((LSTMStateTuple(c=<tf.Tensor 'decoder/while/Exit_3:0' shape=(?, 10) dtype=float32>, h=<tf.Tensor 'decoder/while/Exit_4:0' shape=(?, 10) dtype=float32>),\n",
       "  LSTMStateTuple(c=<tf.Tensor 'decoder/while/Exit_5:0' shape=(?, 10) dtype=float32>, h=<tf.Tensor 'decoder/while/Exit_6:0' shape=(?, 10) dtype=float32>)),\n",
       " <tf.Tensor 'decoder/while/Exit_7:0' shape=(32, 10) dtype=float32>,\n",
       " <tf.Tensor 'decoder/while/Exit_8:0' shape=() dtype=int32>,\n",
       " <tf.Tensor 'decoder/while/Exit_9:0' shape=(32, 16) dtype=float32>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `final_state` has type `tf.contrib.seq2seq.AttentionWrapperState`\n",
    "final_state.cell_state, final_state.attention, final_state.time, final_state.alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decoder/while/Exit_12:0' shape=(32,) dtype=int32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sequence_lengths  # deprecated after TF v1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = final_outputs.rnn_output # float32 [batch_size, sequence_length, num_decoder_symbols]\n",
    "targets = y  # int32 [batch_size, sequence_length]\n",
    "weights = tf.cast(\n",
    "    tf.sequence_mask(y_length, maxlen=y.shape[1]), \n",
    "    tf.float32\n",
    ")  # float32 [batch_size, sequence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits, \n",
    "    targets, \n",
    "    weights,\n",
    "    average_across_timesteps=True,\n",
    "    average_across_batch=True\n",
    ")\n",
    "variable_summaries('loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 1, cross-entropy 4.60423\n",
      "epoch 1, batch 2, cross-entropy 4.60113\n",
      "epoch 1, batch 3, cross-entropy 4.60205\n",
      "epoch 1, batch 4, cross-entropy 4.59707\n",
      "epoch 1, batch 5, cross-entropy 4.59438\n",
      "epoch 1, batch 6, cross-entropy 4.58900\n",
      "epoch 1, batch 7, cross-entropy 4.57931\n",
      "epoch 1, batch 8, cross-entropy 4.57536\n",
      "epoch 1, batch 9, cross-entropy 4.58348\n",
      "epoch 1, batch 10, cross-entropy 4.57630\n",
      "epoch 1, batch 11, cross-entropy 4.56829\n",
      "epoch 1, batch 12, cross-entropy 4.55880\n",
      "epoch 1, batch 13, cross-entropy 4.56774\n",
      "epoch 1, batch 14, cross-entropy 4.57732\n",
      "epoch 1, batch 15, cross-entropy 4.56754\n",
      "epoch 2, batch 1, cross-entropy 4.55250\n",
      "epoch 2, batch 2, cross-entropy 4.56730\n",
      "epoch 2, batch 3, cross-entropy 4.54931\n",
      "epoch 2, batch 4, cross-entropy 4.54116\n",
      "epoch 2, batch 5, cross-entropy 4.54885\n",
      "epoch 2, batch 6, cross-entropy 4.53232\n",
      "epoch 2, batch 7, cross-entropy 4.54544\n",
      "epoch 2, batch 8, cross-entropy 4.54166\n",
      "epoch 2, batch 9, cross-entropy 4.53120\n",
      "epoch 2, batch 10, cross-entropy 4.53006\n",
      "epoch 2, batch 11, cross-entropy 4.53606\n",
      "epoch 2, batch 12, cross-entropy 4.52110\n",
      "epoch 2, batch 13, cross-entropy 4.53003\n",
      "epoch 2, batch 14, cross-entropy 4.52397\n",
      "epoch 2, batch 15, cross-entropy 4.50932\n",
      "epoch 3, batch 1, cross-entropy 4.51796\n",
      "epoch 3, batch 2, cross-entropy 4.51166\n",
      "epoch 3, batch 3, cross-entropy 4.51788\n",
      "epoch 3, batch 4, cross-entropy 4.51639\n",
      "epoch 3, batch 5, cross-entropy 4.51335\n",
      "epoch 3, batch 6, cross-entropy 4.50826\n",
      "epoch 3, batch 7, cross-entropy 4.50057\n",
      "epoch 3, batch 8, cross-entropy 4.49536\n",
      "epoch 3, batch 9, cross-entropy 4.49794\n",
      "epoch 3, batch 10, cross-entropy 4.48670\n",
      "epoch 3, batch 11, cross-entropy 4.48553\n",
      "epoch 3, batch 12, cross-entropy 4.48738\n",
      "epoch 3, batch 13, cross-entropy 4.47398\n",
      "epoch 3, batch 14, cross-entropy 4.47563\n",
      "epoch 3, batch 15, cross-entropy 4.45329\n",
      "epoch 4, batch 1, cross-entropy 4.44776\n",
      "epoch 4, batch 2, cross-entropy 4.43841\n",
      "epoch 4, batch 3, cross-entropy 4.43057\n",
      "epoch 4, batch 4, cross-entropy 4.42056\n",
      "epoch 4, batch 5, cross-entropy 4.40933\n",
      "epoch 4, batch 6, cross-entropy 4.38785\n",
      "epoch 4, batch 7, cross-entropy 4.38214\n",
      "epoch 4, batch 8, cross-entropy 4.37724\n",
      "epoch 4, batch 9, cross-entropy 4.35206\n",
      "epoch 4, batch 10, cross-entropy 4.33516\n",
      "epoch 4, batch 11, cross-entropy 4.31726\n",
      "epoch 4, batch 12, cross-entropy 4.29215\n",
      "epoch 4, batch 13, cross-entropy 4.28267\n",
      "epoch 4, batch 14, cross-entropy 4.26458\n",
      "epoch 4, batch 15, cross-entropy 4.24801\n",
      "epoch 5, batch 1, cross-entropy 4.23879\n",
      "epoch 5, batch 2, cross-entropy 4.23172\n",
      "epoch 5, batch 3, cross-entropy 4.21850\n",
      "epoch 5, batch 4, cross-entropy 4.20724\n",
      "epoch 5, batch 5, cross-entropy 4.18924\n",
      "epoch 5, batch 6, cross-entropy 4.18187\n",
      "epoch 5, batch 7, cross-entropy 4.17659\n",
      "epoch 5, batch 8, cross-entropy 4.16284\n",
      "epoch 5, batch 9, cross-entropy 4.15212\n",
      "epoch 5, batch 10, cross-entropy 4.14804\n",
      "epoch 5, batch 11, cross-entropy 4.14080\n",
      "epoch 5, batch 12, cross-entropy 4.13995\n",
      "epoch 5, batch 13, cross-entropy 4.12657\n",
      "epoch 5, batch 14, cross-entropy 4.11774\n",
      "epoch 5, batch 15, cross-entropy 4.12201\n",
      "epoch 6, batch 1, cross-entropy 4.11028\n",
      "epoch 6, batch 2, cross-entropy 4.11269\n",
      "epoch 6, batch 3, cross-entropy 4.10532\n",
      "epoch 6, batch 4, cross-entropy 4.10397\n",
      "epoch 6, batch 5, cross-entropy 4.10144\n",
      "epoch 6, batch 6, cross-entropy 4.09817\n",
      "epoch 6, batch 7, cross-entropy 4.08492\n",
      "epoch 6, batch 8, cross-entropy 4.08445\n",
      "epoch 6, batch 9, cross-entropy 4.08802\n",
      "epoch 6, batch 10, cross-entropy 4.09106\n",
      "epoch 6, batch 11, cross-entropy 4.07353\n",
      "epoch 6, batch 12, cross-entropy 4.07208\n",
      "epoch 6, batch 13, cross-entropy 4.07027\n",
      "epoch 6, batch 14, cross-entropy 4.07358\n",
      "epoch 6, batch 15, cross-entropy 4.07004\n",
      "epoch 7, batch 1, cross-entropy 4.07694\n",
      "epoch 7, batch 2, cross-entropy 4.05747\n",
      "epoch 7, batch 3, cross-entropy 4.06307\n",
      "epoch 7, batch 4, cross-entropy 4.06340\n",
      "epoch 7, batch 5, cross-entropy 4.05879\n",
      "epoch 7, batch 6, cross-entropy 4.06133\n",
      "epoch 7, batch 7, cross-entropy 4.04484\n",
      "epoch 7, batch 8, cross-entropy 4.05246\n",
      "epoch 7, batch 9, cross-entropy 4.05545\n",
      "epoch 7, batch 10, cross-entropy 4.05462\n",
      "epoch 7, batch 11, cross-entropy 4.04493\n",
      "epoch 7, batch 12, cross-entropy 4.04199\n",
      "epoch 7, batch 13, cross-entropy 4.05056\n",
      "epoch 7, batch 14, cross-entropy 4.03873\n",
      "epoch 7, batch 15, cross-entropy 4.04559\n",
      "epoch 8, batch 1, cross-entropy 4.02522\n",
      "epoch 8, batch 2, cross-entropy 4.03692\n",
      "epoch 8, batch 3, cross-entropy 4.02982\n",
      "epoch 8, batch 4, cross-entropy 4.04261\n",
      "epoch 8, batch 5, cross-entropy 4.02431\n",
      "epoch 8, batch 6, cross-entropy 4.01800\n",
      "epoch 8, batch 7, cross-entropy 4.02222\n",
      "epoch 8, batch 8, cross-entropy 4.02865\n",
      "epoch 8, batch 9, cross-entropy 4.02425\n",
      "epoch 8, batch 10, cross-entropy 4.01049\n",
      "epoch 8, batch 11, cross-entropy 4.00697\n",
      "epoch 8, batch 12, cross-entropy 3.99762\n",
      "epoch 8, batch 13, cross-entropy 4.00324\n",
      "epoch 8, batch 14, cross-entropy 4.00304\n",
      "epoch 8, batch 15, cross-entropy 3.99815\n",
      "epoch 9, batch 1, cross-entropy 3.98909\n",
      "epoch 9, batch 2, cross-entropy 3.99400\n",
      "epoch 9, batch 3, cross-entropy 3.99486\n",
      "epoch 9, batch 4, cross-entropy 3.98550\n",
      "epoch 9, batch 5, cross-entropy 3.97877\n",
      "epoch 9, batch 6, cross-entropy 3.97329\n",
      "epoch 9, batch 7, cross-entropy 3.96949\n",
      "epoch 9, batch 8, cross-entropy 3.96172\n",
      "epoch 9, batch 9, cross-entropy 3.95918\n",
      "epoch 9, batch 10, cross-entropy 3.96624\n",
      "epoch 9, batch 11, cross-entropy 3.96185\n",
      "epoch 9, batch 12, cross-entropy 3.96186\n",
      "epoch 9, batch 13, cross-entropy 3.95336\n",
      "epoch 9, batch 14, cross-entropy 3.94288\n",
      "epoch 9, batch 15, cross-entropy 3.93885\n",
      "epoch 10, batch 1, cross-entropy 3.94700\n",
      "epoch 10, batch 2, cross-entropy 3.93984\n",
      "epoch 10, batch 3, cross-entropy 3.93636\n",
      "epoch 10, batch 4, cross-entropy 3.93996\n",
      "epoch 10, batch 5, cross-entropy 3.93166\n",
      "epoch 10, batch 6, cross-entropy 3.93133\n",
      "epoch 10, batch 7, cross-entropy 3.93234\n",
      "epoch 10, batch 8, cross-entropy 3.92523\n",
      "epoch 10, batch 9, cross-entropy 3.92393\n",
      "epoch 10, batch 10, cross-entropy 3.92105\n",
      "epoch 10, batch 11, cross-entropy 3.91771\n",
      "epoch 10, batch 12, cross-entropy 3.91770\n",
      "epoch 10, batch 13, cross-entropy 3.92613\n",
      "epoch 10, batch 14, cross-entropy 3.91982\n",
      "epoch 10, batch 15, cross-entropy 3.91984\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "logdir = '/root/logs/seq2seq/{:04d}{:02d}{:02d}-{:02d}{:02d}{:02d}'.format(\n",
    "    now.year, now.month, now.day, now.hour, now.minute, now.second\n",
    ")\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # merge all summaries so far and initialize a FileWriter\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(logdir=logdir, graph=sess.graph)\n",
    "    \n",
    "    # initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})\n",
    "\n",
    "    # http://ischlag.github.io/2016/06/19/tensorflow-input-pipeline-example/\n",
    "    # initialize the queue threads to start to shovel data\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    # run epochs\n",
    "    num_batches = n // batch_size\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(num_batches):\n",
    "            sess.run(optimizer)\n",
    "            summary = sess.run(merged)\n",
    "            writer.add_summary(summary, epoch * num_batches + batch)\n",
    "            print('epoch {:d}, batch {:d}, cross-entropy {:.5f}'.format(\n",
    "                epoch+1, batch+1, sess.run(loss)))\n",
    "\n",
    "    # stop our queue threads and properly close the session\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During or after training, summaries can be checked by running `tensorboard --logdir=$LOGDIR_DEFINED_ABOVE` on the command line."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
